{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning en Computer Vision\n",
    "\n",
    "## ğŸ“š Introduction\n",
    "\n",
    "### ğŸ¯ Qu'est-ce que le Fine-Tuning ?\n",
    "\n",
    "Le fine-tuning est une technique d'apprentissage par transfert qui consiste Ã  :\n",
    "\n",
    "1. **Partir d'un modÃ¨le prÃ©-entraÃ®nÃ©** (comme MobileNetV2 entraÃ®nÃ© sur ImageNet)\n",
    "2. **Adapter ce modÃ¨le** Ã  notre problÃ¨me spÃ©cifique\n",
    "3. **RÃ©entraÃ®ner partiellement** le modÃ¨le sur nos donnÃ©es\n",
    "\n",
    "### ğŸ’¡ Pourquoi utiliser le Fine-Tuning ?\n",
    "\n",
    "âœ… **Moins de donnÃ©es nÃ©cessaires** : Utilise les connaissances dÃ©jÃ  apprises\n",
    "\n",
    "âœ… **EntraÃ®nement plus rapide** : Pas besoin de partir de zÃ©ro\n",
    "\n",
    "âœ… **Meilleures performances** : Combine les features gÃ©nÃ©riques et spÃ©cifiques\n",
    "\n",
    "âœ… **Moins de ressources** : Ã‰conomise du temps de calcul\n",
    "\n",
    "### ğŸŒº Notre Projet\n",
    "\n",
    "Dans ce notebook, nous allons :\n",
    "- Classifier 5 types de fleurs diffÃ©rentes\n",
    "- Utiliser le modÃ¨le MobileNetV2 prÃ©-entraÃ®nÃ©\n",
    "- Appliquer deux phases d'entraÃ®nement\n",
    "- Visualiser nos rÃ©sultats Ã  chaque Ã©tape\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Configuration et Imports\n",
    "\n",
    "CommenÃ§ons par importer toutes les bibliothÃ¨ques nÃ©cessaires et configurer notre environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ TensorFlow version: 2.19.0\n",
      "ğŸ’» GPU disponible: âŒ Non\n"
     ]
    }
   ],
   "source": [
    "# Imports principaux\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration pour un meilleur affichage\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(f\"ğŸš€ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"ğŸ’» GPU disponible: {'âœ… Oui' if len(tf.config.list_physical_devices('GPU')) > 0 else 'âŒ Non'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ–¥ï¸ Configuration GPU\n",
    "\n",
    "Si vous avez un GPU, nous allons le configurer pour Ã©viter les erreurs de mÃ©moire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration GPU pour Ã©viter les erreurs OOM (Out of Memory)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Limiter la croissance de la mÃ©moire GPU\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"âœ… Configuration GPU rÃ©ussie - Croissance mÃ©moire activÃ©e\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ Erreur configuration GPU: {e}\")\n",
    "else:\n",
    "    print(\"ğŸ”§ Utilisation du CPU - Pas de GPU dÃ©tectÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ ParamÃ¨tres d'EntraÃ®nement\n",
    "\n",
    "DÃ©finissons les paramÃ¨tres principaux de notre modÃ¨le :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ParamÃ¨tres du modÃ¨le\n",
    "BATCH_SIZE = 32        # Nombre d'images traitÃ©es en mÃªme temps\n",
    "IMG_SIZE = 224         # Taille standard pour MobileNetV2 (224x224 pixels)\n",
    "EPOCHS = 10            # Nombre d'Ã©poques d'entraÃ®nement\n",
    "LEARNING_RATE = 0.0001 # Taux d'apprentissage initial\n",
    "\n",
    "print(\"ğŸ“‹ Configuration d'entraÃ®nement:\")\n",
    "print(f\"   â€¢ Taille de batch: {BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Taille d'image: {IMG_SIZE}x{IMG_SIZE} pixels\")\n",
    "print(f\"   â€¢ Nombre d'Ã©poques: {EPOCHS}\")\n",
    "print(f\"   â€¢ Taux d'apprentissage: {LEARNING_RATE}\")\n",
    "\n",
    "# Explication pÃ©dagogique\n",
    "print(\"\\nğŸ“ Explication des paramÃ¨tres:\")\n",
    "print(\"   ğŸ”¹ BATCH_SIZE: Plus il est grand, plus l'entraÃ®nement est stable mais consomme plus de mÃ©moire\")\n",
    "print(\"   ğŸ”¹ IMG_SIZE: Doit correspondre Ã  la taille attendue par le modÃ¨le prÃ©-entraÃ®nÃ©\")\n",
    "print(\"   ğŸ”¹ LEARNING_RATE: Petit pour le fine-tuning pour Ã©viter de 'casser' les features prÃ©-entraÃ®nÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒº Chargement des DonnÃ©es - Dataset tf_flowers\n",
    "\n",
    "Nous allons utiliser le dataset **tf_flowers** qui contient des images de 5 types de fleurs diffÃ©rentes.\n",
    "\n",
    "### ğŸ“Š Qu'est-ce que tf_flowers ?\n",
    "\n",
    "- **5 classes** de fleurs : daisy, dandelion, roses, sunflowers, tulips\n",
    "- **~3700 images** au total\n",
    "- **Images de tailles variables** (nous les redimensionnerons)\n",
    "- **Dataset supervisÃ©** (chaque image a son label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŒ¸ Chargement du dataset tf_flowers\n",
    "print(\"ğŸ“¥ TÃ©lÃ©chargement du dataset tf_flowers...\")\n",
    "print(\"(Cela peut prendre quelques minutes la premiÃ¨re fois)\\n\")\n",
    "\n",
    "dataset_name = \"tf_flowers\"\n",
    "(ds_train, ds_val), ds_info = tfds.load(\n",
    "    dataset_name,\n",
    "    split=['train[:80%]', 'train[80%:]'],  # 80% pour l'entraÃ®nement, 20% pour la validation\n",
    "    with_info=True,\n",
    "    as_supervised=True  # Retourne (image, label) directement\n",
    ")\n",
    "\n",
    "# ğŸ“Š Informations sur le dataset\n",
    "num_classes = ds_info.features['label'].num_classes\n",
    "class_names = ds_info.features['label'].names\n",
    "\n",
    "print(f\"âœ… Dataset chargÃ© avec succÃ¨s !\")\n",
    "print(f\"ğŸ“ˆ Nombre de classes: {num_classes}\")\n",
    "print(f\"ğŸ·ï¸ Noms des classes: {class_names}\")\n",
    "print(f\"ğŸ“Š Total d'exemples: {ds_info.splits['train'].num_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Exploration du Dataset\n",
    "\n",
    "Regardons la rÃ©partition des donnÃ©es et comprenons la structure :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Compter les Ã©chantillons dans chaque split\n",
    "train_size = sum(1 for _ in ds_train)\n",
    "val_size = sum(1 for _ in ds_val)\n",
    "\n",
    "print(\"ğŸ“‹ RÃ©partition des donnÃ©es:\")\n",
    "print(f\"   ğŸ¯ EntraÃ®nement: {train_size} images ({train_size/(train_size+val_size)*100:.1f}%)\")\n",
    "print(f\"   âœ… Validation: {val_size} images ({val_size/(train_size+val_size)*100:.1f}%)\")\n",
    "print(f\"   ğŸ“Š Total: {train_size + val_size} images\")\n",
    "\n",
    "# ğŸ–¼ï¸ Examiner une image d'exemple\n",
    "print(\"\\nğŸ” Exemple d'image du dataset:\")\n",
    "for image, label in ds_train.take(1):\n",
    "    print(f\"   ğŸ“ Forme de l'image: {image.shape}\")\n",
    "    print(f\"   ğŸ¨ Type de donnÃ©es: {image.dtype}\")\n",
    "    print(f\"   ğŸ·ï¸ Label: {label.numpy()} ({class_names[label.numpy()]})\")\n",
    "    print(f\"   ğŸ“Š Valeurs pixel min/max: {tf.reduce_min(image).numpy()}/{tf.reduce_max(image).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ PrÃ©paration des DonnÃ©es\n",
    "\n",
    "Avant d'entraÃ®ner notre modÃ¨le, nous devons prÃ©parer nos donnÃ©es :\n",
    "\n",
    "1. **Redimensionner** les images Ã  224x224 (taille attendue par MobileNetV2)\n",
    "2. **Normaliser** les pixels selon MobileNetV2\n",
    "3. **Augmenter** les donnÃ©es d'entraÃ®nement\n",
    "4. **Optimiser** les performances avec batching et prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Fonction de prÃ©processing\n",
    "def preprocess_image(image, label):\n",
    "    \"\"\"\n",
    "    PrÃ©processe une image pour MobileNetV2\n",
    "    \n",
    "    Args:\n",
    "        image: Tensor d'image (hauteur, largeur, 3)\n",
    "        label: Label de l'image\n",
    "    \n",
    "    Returns:\n",
    "        image: Image prÃ©processÃ©e (224, 224, 3)\n",
    "        label: Label inchangÃ©\n",
    "    \"\"\"\n",
    "    # âœ‚ï¸ Redimensionnement Ã  224x224\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    # ğŸ¨ PrÃ©processing spÃ©cifique Ã  MobileNetV2\n",
    "    # Convertit les pixels de [0, 255] vers [-1, 1]\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "print(\"âœ… Fonction de prÃ©processing dÃ©finie\")\n",
    "print(\"   ğŸ”„ Redimensionnement: Variable â†’ 224x224\")\n",
    "print(\"   ğŸ“Š Normalisation: [0, 255] â†’ [-1, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ² Data Augmentation\n",
    "\n",
    "L'augmentation de donnÃ©es aide Ã  amÃ©liorer la gÃ©nÃ©ralisation en crÃ©ant des variations des images d'entraÃ®nement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ² Couches d'augmentation de donnÃ©es\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),        # Retournement horizontal alÃ©atoire\n",
    "    layers.RandomRotation(0.2),             # Rotation alÃ©atoire Â±20%\n",
    "    layers.RandomZoom(0.1),                 # Zoom alÃ©atoire Â±10%\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "def augment_image(image, label):\n",
    "    \"\"\"\n",
    "    Applique l'augmentation de donnÃ©es pendant l'entraÃ®nement\n",
    "    \"\"\"\n",
    "    image = data_augmentation(image, training=True)\n",
    "    return image, label\n",
    "\n",
    "print(\"ğŸ² Augmentation de donnÃ©es configurÃ©e:\")\n",
    "print(\"   â†”ï¸ Retournement horizontal alÃ©atoire\")\n",
    "print(\"   ğŸ”„ Rotation alÃ©atoire (Â±36Â°)\")\n",
    "print(\"   ğŸ” Zoom alÃ©atoire (Â±10%)\")\n",
    "print(\"\\nğŸ’¡ Pourquoi augmenter les donnÃ©es ?\")\n",
    "print(\"   â€¢ Augmente artificiellement la taille du dataset\")\n",
    "print(\"   â€¢ AmÃ©liore la gÃ©nÃ©ralisation du modÃ¨le\")\n",
    "print(\"   â€¢ RÃ©duit le surapprentissage (overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Application du Pipeline de DonnÃ©es\n",
    "\n",
    "Maintenant, appliquons toutes nos transformations aux datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ Configuration pour l'optimisation des performances\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "print(\"ğŸ”§ Application du pipeline de prÃ©paration des donnÃ©es...\\n\")\n",
    "\n",
    "# ğŸ¯ Dataset d'entraÃ®nement avec augmentation\n",
    "print(\"ğŸ“‹ Pipeline d'entraÃ®nement:\")\n",
    "ds_train_processed = ds_train.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "print(\"   âœ… 1. PrÃ©processing appliquÃ©\")\n",
    "\n",
    "ds_train_processed = ds_train_processed.map(augment_image, num_parallel_calls=AUTOTUNE)\n",
    "print(\"   âœ… 2. Augmentation appliquÃ©e\")\n",
    "\n",
    "ds_train_processed = ds_train_processed.batch(BATCH_SIZE)\n",
    "print(\"   âœ… 3. Batching appliquÃ©\")\n",
    "\n",
    "ds_train_processed = ds_train_processed.prefetch(AUTOTUNE)\n",
    "print(\"   âœ… 4. Prefetching appliquÃ©\")\n",
    "\n",
    "# ğŸ¯ Dataset de validation SANS augmentation\n",
    "print(\"\\nğŸ“‹ Pipeline de validation:\")\n",
    "ds_val_processed = ds_val.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "print(\"   âœ… 1. PrÃ©processing appliquÃ© (SANS augmentation)\")\n",
    "\n",
    "ds_val_processed = ds_val_processed.batch(BATCH_SIZE)\n",
    "print(\"   âœ… 2. Batching appliquÃ©\")\n",
    "\n",
    "ds_val_processed = ds_val_processed.prefetch(AUTOTUNE)\n",
    "print(\"   âœ… 3. Prefetching appliquÃ©\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Pourquoi pas d'augmentation en validation ?\")\n",
    "print(\"   â€¢ La validation doit Ãªtre reproductible\")\n",
    "print(\"   â€¢ Nous voulons Ã©valuer sur les donnÃ©es 'rÃ©elles'\")\n",
    "print(\"   â€¢ L'augmentation ne sert qu'Ã  enrichir l'entraÃ®nement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ Visualisation des DonnÃ©es\n",
    "\n",
    "Visualisons nos donnÃ©es avant et aprÃ¨s prÃ©processing pour bien comprendre les transformations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset, title=\"Batch d'images\", num_images=9, figsize=(12, 12)):\n",
    "    \"\"\"\n",
    "    Visualise un batch d'images avec leurs labels\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset TensorFlow\n",
    "        title: Titre du graphique\n",
    "        num_images: Nombre d'images Ã  afficher\n",
    "        figsize: Taille de la figure\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(min(num_images, len(images))):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            \n",
    "            # RÃ©cupÃ©rer et prÃ©parer l'image pour l'affichage\n",
    "            image = images[i].numpy()\n",
    "            \n",
    "            # Si l'image est normalisÃ©e [-1, 1], la ramener Ã  [0, 1]\n",
    "            if image.min() < 0:\n",
    "                image = (image + 1) / 2\n",
    "            \n",
    "            # Si les valeurs sont > 1, normaliser\n",
    "            if image.max() > 1:\n",
    "                image = image / 255.0\n",
    "            \n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"{class_names[labels[i]]}\", fontsize=12, pad=10)\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ğŸ¨ Fonction de visualisation crÃ©Ã©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ–¼ï¸ Images Originales\n",
    "\n",
    "Regardons d'abord les images telles qu'elles sont dans le dataset original :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ–¼ï¸ Fonction de redimensionnement simple pour la visualisation\n",
    "def resize_for_display(image, label):\n",
    "    \"\"\"Redimensionne uniquement pour l'affichage (pas de normalisation)\"\"\"\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label\n",
    "\n",
    "# ğŸ–¼ï¸ Affichage des images originales (redimensionnÃ©es pour l'affichage)\n",
    "print(\"ğŸŒ¸ AperÃ§u des images originales du dataset:\")\n",
    "print(\"â„¹ï¸ Note: Images redimensionnÃ©es Ã  224x224 pour permettre l'affichage en batch\")\n",
    "ds_train_display = ds_train.map(resize_for_display).batch(9)\n",
    "show_batch(ds_train_display, title=\"Images Originales (redimensionnÃ©es pour affichage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ Images AprÃ¨s PrÃ©processing\n",
    "\n",
    "Maintenant, voyons les mÃªmes images aprÃ¨s redimensionnement et normalisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ Affichage aprÃ¨s prÃ©processing (sans augmentation)\n",
    "ds_preprocessed_only = ds_train.map(preprocess_image).batch(9)\n",
    "print(\"ğŸ”§ AperÃ§u aprÃ¨s prÃ©processing (redimensionnement + normalisation):\")\n",
    "show_batch(ds_preprocessed_only, title=\"Images AprÃ¨s PrÃ©processing (224x224, normalisÃ©es)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ² Effet de l'Augmentation de DonnÃ©es\n",
    "\n",
    "Comparons la mÃªme image avant et aprÃ¨s augmentation pour voir l'effet des transformations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ² DÃ©monstration de l'augmentation de donnÃ©es\n",
    "def show_augmentation_effect():\n",
    "    \"\"\"\n",
    "    Montre l'effet de l'augmentation sur une mÃªme image\n",
    "    \"\"\"\n",
    "    # Prendre une seule image\n",
    "    for original_image, label in ds_train.take(1):\n",
    "        # PrÃ©processer l'image\n",
    "        processed_image, _ = preprocess_image(original_image, label)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        fig.suptitle(f\"Effet de l'Augmentation de DonnÃ©es - Classe: {class_names[label]}\", \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Image originale prÃ©processÃ©e\n",
    "        axes[0, 0].imshow((processed_image + 1) / 2)\n",
    "        axes[0, 0].set_title(\"Image Originale\\n(aprÃ¨s prÃ©processing)\", fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # GÃ©nÃ©rer plusieurs versions augmentÃ©es\n",
    "        for i in range(1, 4):\n",
    "            augmented_image, _ = augment_image(processed_image, label)\n",
    "            axes[0, i].imshow((augmented_image + 1) / 2)\n",
    "            axes[0, i].set_title(f\"Augmentation {i}\")\n",
    "            axes[0, i].axis('off')\n",
    "        \n",
    "        # Encore plus d'exemples\n",
    "        for i in range(4):\n",
    "            augmented_image, _ = augment_image(processed_image, label)\n",
    "            axes[1, i].imshow((augmented_image + 1) / 2)\n",
    "            axes[1, i].set_title(f\"Augmentation {i+4}\")\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "print(\"ğŸ² DÃ©monstration de l'augmentation de donnÃ©es:\")\n",
    "show_augmentation_effect()\n",
    "\n",
    "print(\"\\nğŸ’¡ Observations sur l'augmentation:\")\n",
    "print(\"   ğŸ”„ Chaque version est lÃ©gÃ¨rement diffÃ©rente\")\n",
    "print(\"   ğŸ“Š Cela multiplie artificiellement notre dataset\")\n",
    "print(\"   ğŸ¯ Le modÃ¨le apprend Ã  Ãªtre plus robuste aux variations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š AperÃ§u Final des DonnÃ©es d'EntraÃ®nement\n",
    "\n",
    "Regardons maintenant un batch complet tel qu'il sera fourni au modÃ¨le :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Affichage du dataset final d'entraÃ®nement\n",
    "print(\"ğŸ¯ AperÃ§u final du dataset d'entraÃ®nement (avec toutes les transformations):\")\n",
    "show_batch(ds_train_processed, title=\"Dataset d'EntraÃ®nement Final (prÃ©processÃ© + augmentÃ©)\")\n",
    "\n",
    "print(\"\\nğŸ“‹ RÃ©sumÃ© de ce que le modÃ¨le va recevoir:\")\n",
    "print(\"   ğŸ“ Taille: 224x224x3 pixels\")\n",
    "print(\"   ğŸ“Š Valeurs: [-1, 1] (normalisÃ©es pour MobileNetV2)\")\n",
    "print(\"   ğŸ² VariÃ©tÃ©: AugmentÃ©es alÃ©atoirement Ã  chaque Ã©poque\")\n",
    "print(\"   ğŸ“¦ Batch: 32 images Ã  la fois\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  CrÃ©ation du ModÃ¨le avec Transfer Learning\n",
    "\n",
    "Maintenant que nos donnÃ©es sont prÃªtes, crÃ©ons notre modÃ¨le basÃ© sur MobileNetV2 prÃ©-entraÃ®nÃ©.\n",
    "\n",
    "### ğŸ¯ Qu'est-ce que MobileNetV2 ?\n",
    "\n",
    "- **Architecture lÃ©gÃ¨re** optimisÃ©e pour mobile\n",
    "- **PrÃ©-entraÃ®nÃ© sur ImageNet** (1.4M d'images, 1000 classes)\n",
    "- **Excellent Ã©quilibre** entre performance et vitesse\n",
    "- **IdÃ©al pour le fine-tuning** sur de nouveaux domaines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Chargement du modÃ¨le de base MobileNetV2\n",
    "print(\"ğŸš€ Chargement de MobileNetV2 prÃ©-entraÃ®nÃ©...\")\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),  # Forme d'entrÃ©e\n",
    "    include_top=False,                    # Exclure la couche de classification finale\n",
    "    weights='imagenet'                    # Utiliser les poids prÃ©-entraÃ®nÃ©s ImageNet\n",
    ")\n",
    "\n",
    "# ğŸ”’ Geler les couches du modÃ¨le de base (transfer learning)\n",
    "base_model.trainable = False\n",
    "\n",
    "print(\"âœ… MobileNetV2 chargÃ© avec succÃ¨s !\")\n",
    "print(f\"ğŸ“Š Nombre de couches: {len(base_model.layers)}\")\n",
    "print(f\"ğŸ“ Forme de sortie: {base_model.output_shape}\")\n",
    "print(f\"ğŸ”’ Couches gelÃ©es: {not base_model.trainable}\")\n",
    "\n",
    "# ğŸ’¡ Explication pÃ©dagogique\n",
    "print(\"\\nğŸ“ Pourquoi geler les couches ?\")\n",
    "print(\"   â€¢ Les features bas niveau (bords, textures) sont dÃ©jÃ  bien apprises\")\n",
    "print(\"   â€¢ Ã‰vite de 'casser' les reprÃ©sentations prÃ©-entraÃ®nÃ©es\")\n",
    "print(\"   â€¢ AccÃ©lÃ¨re l'entraÃ®nement initial\")\n",
    "print(\"   â€¢ Nous dÃ©gelerons certaines couches plus tard (fine-tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ—ï¸ Architecture ComplÃ¨te du ModÃ¨le\n",
    "\n",
    "Ajoutons nos propres couches au-dessus du modÃ¨le prÃ©-entraÃ®nÃ© :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ Construction du modÃ¨le complet\n",
    "print(\"ğŸ”§ Construction de l'architecture complÃ¨te...\")\n",
    "\n",
    "# Couche d'entrÃ©e\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# ModÃ¨le de base (gelÃ©)\n",
    "x = base_model(inputs, training=False)  # training=False pour utiliser en mode infÃ©rence\n",
    "\n",
    "# Pooling global pour rÃ©duire les dimensions\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Couche de dropout pour la rÃ©gularisation\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# Couche de classification finale\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# CrÃ©ation du modÃ¨le final\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "print(\"âœ… ModÃ¨le crÃ©Ã© avec succÃ¨s !\")\n",
    "\n",
    "# Calculer les paramÃ¨tres de maniÃ¨re compatible TensorFlow\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum(tf.size(var).numpy() for var in model.trainable_variables)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"ğŸ“Š Nombre total de paramÃ¨tres: {total_params:,}\")\n",
    "print(f\"ğŸ¯ ParamÃ¨tres entraÃ®nables: {trainable_params:,}\")\n",
    "print(f\"ğŸ”’ ParamÃ¨tres gelÃ©s: {frozen_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“‹ RÃ©sumÃ© de l'Architecture\n",
    "\n",
    "Visualisons l'architecture de notre modÃ¨le :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‹ Affichage du rÃ©sumÃ© du modÃ¨le\n",
    "print(\"ğŸ—ï¸ Architecture du modÃ¨le:\")\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nğŸ“ Explication de l'architecture:\")\n",
    "print(\"   ğŸ“¥ Input (224, 224, 3): Image d'entrÃ©e\")\n",
    "print(\"   ğŸ§  MobileNetV2: Extraction de features (gelÃ©)\")\n",
    "print(\"   ğŸŒ GlobalAveragePooling2D: RÃ©duction dimensionnelle\")\n",
    "print(\"   ğŸ² Dropout(0.2): RÃ©gularisation (20% de neurones dÃ©sactivÃ©s)\")\n",
    "print(\"   ğŸ¯ Dense(5, softmax): Classification finale (5 classes de fleurs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Visualisation de l'Architecture\n",
    "\n",
    "CrÃ©ons un diagramme pour mieux comprendre notre modÃ¨le :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¨ Visualisation de l'architecture\n",
    "def visualize_model_architecture():\n",
    "    \"\"\"\n",
    "    CrÃ©e un diagramme explicatif de l'architecture\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "    \n",
    "    # DÃ©finir les composants et leurs positions\n",
    "    components = [\n",
    "        {\"name\": \"Image d'entrÃ©e\\n224Ã—224Ã—3\", \"pos\": (1, 8), \"color\": \"lightblue\", \"frozen\": False},\n",
    "        {\"name\": \"MobileNetV2\\n(PrÃ©-entraÃ®nÃ©)\\nğŸ”’ GELÃ‰\", \"pos\": (1, 6), \"color\": \"lightcoral\", \"frozen\": True},\n",
    "        {\"name\": \"Features\\n7Ã—7Ã—1280\", \"pos\": (1, 4.5), \"color\": \"lightgreen\", \"frozen\": True},\n",
    "        {\"name\": \"GlobalAveragePooling2D\\nâ†’ 1280\", \"pos\": (1, 3), \"color\": \"lightyellow\", \"frozen\": False},\n",
    "        {\"name\": \"Dropout(0.2)\\nğŸ² RÃ©gularisation\", \"pos\": (1, 1.5), \"color\": \"lightpink\", \"frozen\": False},\n",
    "        {\"name\": \"Dense(5)\\n+ Softmax\\nğŸ¯ Classification\", \"pos\": (1, 0), \"color\": \"lightsteelblue\", \"frozen\": False}\n",
    "    ]\n",
    "    \n",
    "    # Dessiner les composants\n",
    "    for i, comp in enumerate(components):\n",
    "        x, y = comp[\"pos\"]\n",
    "        \n",
    "        # Style diffÃ©rent pour les couches gelÃ©es\n",
    "        if comp[\"frozen\"]:\n",
    "            bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=comp[\"color\"], \n",
    "                            edgecolor=\"red\", linewidth=2, linestyle=\"--\")\n",
    "        else:\n",
    "            bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=comp[\"color\"], \n",
    "                            edgecolor=\"black\", linewidth=1)\n",
    "        \n",
    "        ax.text(x, y, comp[\"name\"], ha=\"center\", va=\"center\", \n",
    "               fontsize=12, fontweight=\"bold\", bbox=bbox_props)\n",
    "        \n",
    "        # FlÃ¨ches entre composants\n",
    "        if i < len(components) - 1:\n",
    "            next_y = components[i+1][\"pos\"][1]\n",
    "            ax.annotate(\"\", xy=(x, next_y + 0.4), xytext=(x, y - 0.4),\n",
    "                       arrowprops=dict(arrowstyle=\"->\", lw=2, color=\"darkblue\"))\n",
    "    \n",
    "    # Annotations explicatives\n",
    "    ax.text(3, 6, \"Phase 1: Transfer Learning\\nâ€¢ Couches gelÃ©es\\nâ€¢ Apprentissage rapide\\nâ€¢ Features gÃ©nÃ©riques\", \n",
    "           fontsize=11, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"wheat\", alpha=0.8))\n",
    "    \n",
    "    ax.text(3, 2, \"Phase 2: Fine-Tuning\\nâ€¢ DÃ©gel partiel\\nâ€¢ Apprentissage lent\\nâ€¢ Adaptation spÃ©cifique\", \n",
    "           fontsize=11, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcyan\", alpha=0.8))\n",
    "    \n",
    "    # Configuration des axes\n",
    "    ax.set_xlim(-0.5, 5)\n",
    "    ax.set_ylim(-1, 9)\n",
    "    ax.set_title(\"Architecture du ModÃ¨le de Fine-Tuning\", fontsize=16, fontweight=\"bold\", pad=20)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # LÃ©gende\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightcoral', edgecolor='red', linestyle='--', label='Couches gelÃ©es'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightsteelblue', edgecolor='black', label='Couches entraÃ®nables')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ğŸ¨ Visualisation de l'architecture du modÃ¨le:\")\n",
    "visualize_model_architecture()\n",
    "\n",
    "print(\"\\nğŸ’¡ Points clÃ©s de l'architecture:\")\n",
    "print(\"   ğŸ”’ MobileNetV2 gelÃ©: PrÃ©serve les features prÃ©-apprises\")\n",
    "print(\"   ğŸ¯ Nouvelles couches: AdaptÃ©es Ã  notre problÃ¨me (5 classes)\")\n",
    "print(\"   âš¡ Efficace: Peu de paramÃ¨tres Ã  entraÃ®ner initialement\")\n",
    "print(\"   ğŸ² RÃ©gularisation: Dropout pour Ã©viter l'overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Phase 1 : Transfer Learning (Couches GelÃ©es)\n",
    "\n",
    "Nous allons maintenant entraÃ®ner notre modÃ¨le en deux phases :\n",
    "\n",
    "1. **Phase 1** : Transfer Learning avec couches gelÃ©es\n",
    "2. **Phase 2** : Fine-Tuning avec dÃ©gel partiel\n",
    "\n",
    "### ğŸ”’ Pourquoi Commencer avec les Couches GelÃ©es ?\n",
    "\n",
    "- âš¡ **EntraÃ®nement rapide** : Moins de paramÃ¨tres Ã  mettre Ã  jour\n",
    "- ğŸ›¡ï¸ **PrÃ©servation des features** : Ã‰vite de \"casser\" les reprÃ©sentations prÃ©-apprises\n",
    "- ğŸ¯ **Adaptation douce** : Les nouvelles couches apprennent d'abord Ã  utiliser les features existantes\n",
    "- ğŸ’¾ **Ã‰conomie de ressources** : Moins de mÃ©moire et calculs nÃ©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Configuration de l'entraÃ®nement Phase 1\n",
    "print(\"ğŸ¯ === PHASE 1: TRANSFER LEARNING (couches gelÃ©es) ===\")\n",
    "print(\"\\nğŸ”§ Configuration de l'optimiseur et des mÃ©triques...\")\n",
    "\n",
    "# Compilation du modÃ¨le\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='sparse_categorical_crossentropy',  # Pour labels entiers (0, 1, 2, 3, 4)\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… ModÃ¨le compilÃ© !\")\n",
    "print(f\"   ğŸ§  Optimiseur: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"   ğŸ“Š Fonction de perte: Sparse Categorical Crossentropy\")\n",
    "print(f\"   ğŸ“ˆ MÃ©triques: Accuracy\")\n",
    "\n",
    "# ğŸ“ Explication pÃ©dagogique\n",
    "print(\"\\nğŸ“ Pourquoi ces choix ?\")\n",
    "print(\"   â€¢ Adam: Optimiseur adaptatif, bon pour la plupart des cas\")\n",
    "print(\"   â€¢ Sparse Categorical: Nos labels sont des entiers (0,1,2,3,4)\")\n",
    "print(\"   â€¢ Learning rate faible: Pour ne pas perturber les features prÃ©-entraÃ®nÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Callbacks pour l'EntraÃ®nement\n",
    "\n",
    "Les callbacks nous aident Ã  surveiller et contrÃ´ler l'entraÃ®nement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Configuration des callbacks\n",
    "callbacks = [\n",
    "    # RÃ©duction du learning rate si la perte stagne\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',           # Surveiller la perte de validation\n",
    "        factor=0.2,                   # Diviser le LR par 5 (Ã—0.2)\n",
    "        patience=3,                   # Attendre 3 Ã©poques sans amÃ©lioration\n",
    "        min_lr=1e-7,                  # LR minimum\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # ArrÃªt anticipÃ© si pas d'amÃ©lioration\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',           # Surveiller la perte de validation\n",
    "        patience=5,                   # Attendre 5 Ã©poques sans amÃ©lioration\n",
    "        restore_best_weights=True,    # Restaurer les meilleurs poids\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"ğŸ”§ Callbacks configurÃ©s:\")\n",
    "print(\"   ğŸ“‰ ReduceLROnPlateau: RÃ©duit automatiquement le learning rate\")\n",
    "print(\"   â¹ï¸ EarlyStopping: ArrÃªte l'entraÃ®nement si pas d'amÃ©lioration\")\n",
    "print(\"\\nğŸ’¡ Avantages des callbacks:\")\n",
    "print(\"   â€¢ Ã‰vitent le surapprentissage\")\n",
    "print(\"   â€¢ Ã‰conomisent du temps de calcul\")\n",
    "print(\"   â€¢ Trouvent automatiquement les meilleurs hyperparamÃ¨tres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ Lancement de l'EntraÃ®nement Phase 1\n",
    "\n",
    "C'est parti pour la premiÃ¨re phase d'entraÃ®nement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ EntraÃ®nement Phase 1\n",
    "print(\"ğŸš€ DÃ©but de l'entraÃ®nement Phase 1...\")\n",
    "print(f\"ğŸ“Š {sum(1 for _ in ds_train_processed)} batches d'entraÃ®nement\")\n",
    "print(f\"ğŸ“Š {sum(1 for _ in ds_val_processed)} batches de validation\")\n",
    "print(\"\\nâ° Cela peut prendre quelques minutes...\\n\")\n",
    "\n",
    "# EntraÃ®nement\n",
    "history = model.fit(\n",
    "    ds_train_processed,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=ds_val_processed,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ‰ Phase 1 terminÃ©e !\")\n",
    "print(f\"âœ… Nombre d'Ã©poques effectuÃ©es: {len(history.history['accuracy'])}\")\n",
    "print(f\"ğŸ“ˆ PrÃ©cision finale d'entraÃ®nement: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"ğŸ“Š PrÃ©cision finale de validation: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”“ Phase 2 : Fine-Tuning (DÃ©gel Partiel)\n",
    "\n",
    "Maintenant que notre modÃ¨le a appris Ã  utiliser les features de MobileNetV2, nous allons **dÃ©geler** certaines couches pour un ajustement plus fin.\n",
    "\n",
    "### ğŸ¯ StratÃ©gie de DÃ©gel\n",
    "\n",
    "- ğŸ”“ **DÃ©geler les derniÃ¨res couches** : Elles contiennent les features plus spÃ©cifiques\n",
    "- ğŸ”’ **Garder gelÃ©es les premiÃ¨res couches** : Elles contiennent les features gÃ©nÃ©riques (bords, textures)\n",
    "- ğŸ“‰ **RÃ©duire le learning rate** : Pour des ajustements plus fins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Analyse des couches de MobileNetV2\n",
    "print(\"ğŸ” Analyse des couches de MobileNetV2:\")\n",
    "print(f\"ğŸ“Š Nombre total de couches: {len(base_model.layers)}\")\n",
    "\n",
    "# Afficher quelques couches pour comprendre la structure\n",
    "print(\"\\nğŸ—ï¸ Structure des couches (derniÃ¨res 10):\")\n",
    "for i, layer in enumerate(base_model.layers[-10:]):\n",
    "    print(f\"   {len(base_model.layers)-10+i:3d}: {layer.name} ({layer.__class__.__name__})\")\n",
    "\n",
    "# ğŸ”“ StratÃ©gie de dÃ©gel : dÃ©geler les 20 derniÃ¨res couches\n",
    "fine_tune_at = len(base_model.layers) - 20\n",
    "\n",
    "print(f\"\\nğŸ¯ StratÃ©gie de dÃ©gel:\")\n",
    "print(f\"   ğŸ”’ Couches 0 Ã  {fine_tune_at-1}: GELÃ‰ES (features gÃ©nÃ©riques)\")\n",
    "print(f\"   ğŸ”“ Couches {fine_tune_at} Ã  {len(base_model.layers)-1}: DÃ‰GELÃ‰ES (features spÃ©cifiques)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”“ Application du dÃ©gel\n",
    "print(\"ğŸ”§ Application de la stratÃ©gie de dÃ©gel...\")\n",
    "\n",
    "# DÃ©geler le modÃ¨le de base\n",
    "base_model.trainable = True\n",
    "\n",
    "# Geler les premiÃ¨res couches, dÃ©geler les derniÃ¨res\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    if i < fine_tune_at:\n",
    "        layer.trainable = False  # Garder gelÃ©\n",
    "    else:\n",
    "        layer.trainable = True   # DÃ©geler\n",
    "\n",
    "# Compter les paramÃ¨tres entraÃ®nables\n",
    "trainable_params = sum(tf.size(var).numpy() for var in model.trainable_variables)\n",
    "total_params = model.count_params()\n",
    "\n",
    "print(\"âœ… DÃ©gel appliquÃ© !\")\n",
    "print(f\"ğŸ“Š ParamÃ¨tres entraÃ®nables: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
    "print(f\"ğŸ”’ ParamÃ¨tres gelÃ©s: {total_params-trainable_params:,} ({(total_params-trainable_params)/total_params*100:.1f}%)\")\n",
    "print(f\"ğŸ“ˆ Total: {total_params:,} paramÃ¨tres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ Recompilation avec Learning Rate RÃ©duit\n",
    "\n",
    "Pour le fine-tuning, nous utilisons un learning rate plus faible :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Recompilation avec learning rate rÃ©duit\n",
    "fine_tune_lr = LEARNING_RATE / 10  # Diviser par 10\n",
    "\n",
    "print(f\"ğŸ”§ Recompilation du modÃ¨le pour le fine-tuning...\")\n",
    "print(f\"ğŸ“‰ Nouveau learning rate: {fine_tune_lr} (10x plus faible)\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… ModÃ¨le recompilÃ© !\")\n",
    "print(\"\\nğŸ’¡ Pourquoi un learning rate plus faible ?\")\n",
    "print(\"   â€¢ Les features prÃ©-entraÃ®nÃ©es sont dÃ©jÃ  bonnes\")\n",
    "print(\"   â€¢ On veut des ajustements fins, pas des changements drastiques\")\n",
    "print(\"   â€¢ Ã‰vite de 'casser' ce qui a Ã©tÃ© appris en Phase 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ Lancement du Fine-Tuning\n",
    "\n",
    "Continuons l'entraÃ®nement avec les couches dÃ©gelÃ©es :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ EntraÃ®nement Phase 2 (Fine-tuning)\n",
    "print(\"ğŸš€ === PHASE 2: FINE-TUNING (couches dÃ©gelÃ©es) ===\")\n",
    "print(\"\\nâ° DÃ©but du fine-tuning...\")\n",
    "\n",
    "# Continuer l'entraÃ®nement depuis la derniÃ¨re Ã©poque\n",
    "initial_epoch = len(history.history['accuracy'])\n",
    "total_epochs = initial_epoch + EPOCHS\n",
    "\n",
    "print(f\"ğŸ“Š Reprendre depuis l'Ã©poque: {initial_epoch}\")\n",
    "print(f\"ğŸ¯ Aller jusqu'Ã  l'Ã©poque: {total_epochs}\")\n",
    "print(f\"â• Ã‰poque supplÃ©mentaires: {EPOCHS}\")\n",
    "\n",
    "# Fine-tuning\n",
    "history_fine = model.fit(\n",
    "    ds_train_processed,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=initial_epoch,\n",
    "    validation_data=ds_val_processed,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ‰ Fine-tuning terminÃ© !\")\n",
    "print(f\"âœ… Ã‰poques totales effectuÃ©es: {len(history.history['accuracy']) + len(history_fine.history['accuracy'])}\")\n",
    "print(f\"ğŸ“ˆ PrÃ©cision finale d'entraÃ®nement: {history_fine.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"ğŸ“Š PrÃ©cision finale de validation: {history_fine.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Visualisation des RÃ©sultats d'EntraÃ®nement\n",
    "\n",
    "Analysons maintenant les performances de notre modÃ¨le Ã  travers les deux phases d'entraÃ®nement.\n",
    "\n",
    "### ğŸ“ˆ Importance de la Visualisation\n",
    "\n",
    "Les graphiques nous permettent de :\n",
    "- **DÃ©tecter le surapprentissage** (overfitting)\n",
    "- **Comprendre l'Ã©volution** de l'apprentissage\n",
    "- **Valider l'efficacitÃ©** du fine-tuning\n",
    "- **Identifier les problÃ¨mes** potentiels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history1, history2=None, title=\"Ã‰volution de l'EntraÃ®nement\"):\n",
    "    \"\"\"\n",
    "    Affiche les courbes d'entraÃ®nement pour les deux phases\n",
    "    \n",
    "    Args:\n",
    "        history1: Historique de la Phase 1 (Transfer Learning)\n",
    "        history2: Historique de la Phase 2 (Fine-Tuning) - optionnel\n",
    "        title: Titre du graphique\n",
    "    \"\"\"\n",
    "    # PrÃ©parer les donnÃ©es\n",
    "    if history2:\n",
    "        # Combiner les deux phases\n",
    "        acc = history1.history['accuracy'] + history2.history['accuracy']\n",
    "        val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
    "        loss = history1.history['loss'] + history2.history['loss']\n",
    "        val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
    "        \n",
    "        # Point de transition entre les phases\n",
    "        transition_point = len(history1.history['accuracy']) - 1\n",
    "        \n",
    "        phases_info = True\n",
    "    else:\n",
    "        # Une seule phase\n",
    "        acc = history1.history['accuracy']\n",
    "        val_acc = history1.history['val_accuracy']\n",
    "        loss = history1.history['loss']\n",
    "        val_loss = history1.history['val_loss']\n",
    "        \n",
    "        phases_info = False\n",
    "    \n",
    "    epochs_range = range(len(acc))\n",
    "    \n",
    "    # CrÃ©er la figure avec 2 sous-graphiques\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Graphique 1: PrÃ©cision\n",
    "    ax1.plot(epochs_range, acc, 'b-', label='EntraÃ®nement', linewidth=2)\n",
    "    ax1.plot(epochs_range, val_acc, 'r-', label='Validation', linewidth=2)\n",
    "    \n",
    "    if phases_info:\n",
    "        # Ligne de sÃ©paration entre les phases\n",
    "        ax1.axvline(x=transition_point, color='green', linestyle='--', \n",
    "                   linewidth=2, label='DÃ©but Fine-Tuning')\n",
    "        \n",
    "        # Zones colorÃ©es pour les phases\n",
    "        ax1.axvspan(0, transition_point, alpha=0.1, color='blue', label='Phase 1: Transfer Learning')\n",
    "        ax1.axvspan(transition_point, len(acc)-1, alpha=0.1, color='orange', label='Phase 2: Fine-Tuning')\n",
    "    \n",
    "    ax1.set_title('ğŸ“ˆ Ã‰volution de la PrÃ©cision', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Ã‰poques')\n",
    "    ax1.set_ylabel('PrÃ©cision')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Graphique 2: Perte\n",
    "    ax2.plot(epochs_range, loss, 'b-', label='EntraÃ®nement', linewidth=2)\n",
    "    ax2.plot(epochs_range, val_loss, 'r-', label='Validation', linewidth=2)\n",
    "    \n",
    "    if phases_info:\n",
    "        # Ligne de sÃ©paration entre les phases\n",
    "        ax2.axvline(x=transition_point, color='green', linestyle='--', \n",
    "                   linewidth=2, label='DÃ©but Fine-Tuning')\n",
    "        \n",
    "        # Zones colorÃ©es pour les phases\n",
    "        ax2.axvspan(0, transition_point, alpha=0.1, color='blue', label='Phase 1: Transfer Learning')\n",
    "        ax2.axvspan(transition_point, len(loss)-1, alpha=0.1, color='orange', label='Phase 2: Fine-Tuning')\n",
    "    \n",
    "    ax2.set_title('ğŸ“‰ Ã‰volution de la Perte', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Ã‰poques')\n",
    "    ax2.set_ylabel('Perte')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques finales\n",
    "    print(\"\\nğŸ“Š Statistiques finales:\")\n",
    "    print(f\"   ğŸ¯ PrÃ©cision finale (entraÃ®nement): {acc[-1]:.4f} ({acc[-1]*100:.2f}%)\")\n",
    "    print(f\"   âœ… PrÃ©cision finale (validation): {val_acc[-1]:.4f} ({val_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"   ğŸ“‰ Perte finale (entraÃ®nement): {loss[-1]:.4f}\")\n",
    "    print(f\"   ğŸ“Š Perte finale (validation): {val_loss[-1]:.4f}\")\n",
    "    \n",
    "    if phases_info:\n",
    "        print(f\"\\nğŸ“ˆ AmÃ©lioration grÃ¢ce au fine-tuning:\")\n",
    "        print(f\"   â€¢ PrÃ©cision validation: {val_acc[transition_point]:.4f} â†’ {val_acc[-1]:.4f} \"\n",
    "              f\"(+{(val_acc[-1] - val_acc[transition_point])*100:.2f}%)\")\n",
    "        print(f\"   â€¢ Perte validation: {val_loss[transition_point]:.4f} â†’ {val_loss[-1]:.4f} \"\n",
    "              f\"({((val_loss[-1] - val_loss[transition_point])/val_loss[transition_point])*100:+.2f}%)\")\n",
    "\n",
    "print(\"ğŸ“Š Fonction de visualisation des rÃ©sultats crÃ©Ã©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Visualisation des Performances\n",
    "\n",
    "Affichons maintenant les rÃ©sultats de nos deux phases d'entraÃ®nement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Affichage des courbes de performance\n",
    "print(\"ğŸ“ˆ Visualisation des rÃ©sultats d'entraÃ®nement:\")\n",
    "plot_training_history(history, history_fine, \"ğŸŒ¸ RÃ©sultats du Fine-Tuning - Classification de Fleurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Analyse des RÃ©sultats\n",
    "\n",
    "Analysons ce que nous disent ces graphiques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Analyse automatique des rÃ©sultats\n",
    "def analyze_training_results(history1, history2=None):\n",
    "    \"\"\"\n",
    "    Analyse automatique des rÃ©sultats d'entraÃ®nement\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” === ANALYSE DES RÃ‰SULTATS D'ENTRAÃNEMENT ===\")\n",
    "    \n",
    "    if history2:\n",
    "        # Analyse avec fine-tuning\n",
    "        val_acc_phase1 = history1.history['val_accuracy'][-1]\n",
    "        val_acc_phase2 = history2.history['val_accuracy'][-1]\n",
    "        improvement = val_acc_phase2 - val_acc_phase1\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ Comparaison des phases:\")\n",
    "        print(f\"   Phase 1 (Transfer Learning): {val_acc_phase1:.4f} ({val_acc_phase1*100:.2f}%)\")\n",
    "        print(f\"   Phase 2 (Fine-Tuning): {val_acc_phase2:.4f} ({val_acc_phase2*100:.2f}%)\")\n",
    "        print(f\"   ğŸš€ AmÃ©lioration: +{improvement:.4f} (+{improvement*100:.2f}%)\")\n",
    "        \n",
    "        # Ã‰valuation de l'amÃ©lioration\n",
    "        if improvement > 0.02:\n",
    "            print(\"   âœ… Excellent ! Le fine-tuning a significativement amÃ©liorÃ© les performances\")\n",
    "        elif improvement > 0.01:\n",
    "            print(\"   ğŸ‘ Bien ! Le fine-tuning a apportÃ© une amÃ©lioration notable\")\n",
    "        elif improvement > 0:\n",
    "            print(\"   ğŸ“ˆ Le fine-tuning a lÃ©gÃ¨rement amÃ©liorÃ© les performances\")\n",
    "        else:\n",
    "            print(\"   âš ï¸ Le fine-tuning n'a pas amÃ©liorÃ© les performances (possible surapprentissage)\")\n",
    "    \n",
    "    # Analyse du surapprentissage\n",
    "    final_history = history2 if history2 else history1\n",
    "    train_acc = final_history.history['accuracy'][-1]\n",
    "    val_acc = final_history.history['val_accuracy'][-1]\n",
    "    gap = train_acc - val_acc\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Analyse du surapprentissage:\")\n",
    "    print(f\"   PrÃ©cision entraÃ®nement: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"   PrÃ©cision validation: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    print(f\"   Ã‰cart: {gap:.4f} ({gap*100:.2f}%)\")\n",
    "    \n",
    "    if gap < 0.05:\n",
    "        print(\"   âœ… Excellent Ã©quilibre ! Pas de surapprentissage dÃ©tectÃ©\")\n",
    "    elif gap < 0.10:\n",
    "        print(\"   ğŸ‘ Bon Ã©quilibre, lÃ©ger surapprentissage acceptable\")\n",
    "    elif gap < 0.15:\n",
    "        print(\"   âš ï¸ Surapprentissage modÃ©rÃ© - pourrait Ãªtre amÃ©liorÃ©\")\n",
    "    else:\n",
    "        print(\"   ğŸš¨ Surapprentissage important - rÃ©viser la rÃ©gularisation\")\n",
    "    \n",
    "    # Recommandations\n",
    "    print(f\"\\nğŸ’¡ Recommandations:\")\n",
    "    if gap > 0.10:\n",
    "        print(\"   â€¢ Augmenter le dropout\")\n",
    "        print(\"   â€¢ Ajouter plus d'augmentation de donnÃ©es\")\n",
    "        print(\"   â€¢ RÃ©duire le learning rate\")\n",
    "        print(\"   â€¢ Utiliser l'early stopping plus agressivement\")\n",
    "    \n",
    "    if val_acc < 0.80:\n",
    "        print(\"   â€¢ Essayer un modÃ¨le plus large (ResNet, EfficientNet)\")\n",
    "        print(\"   â€¢ Augmenter le nombre d'Ã©poques\")\n",
    "        print(\"   â€¢ DÃ©geler plus de couches\")\n",
    "    \n",
    "    if val_acc > 0.90:\n",
    "        print(\"   â€¢ Excellents rÃ©sultats ! Le modÃ¨le est prÃªt pour la production\")\n",
    "        print(\"   â€¢ ConsidÃ©rer tester sur un dataset externe\")\n",
    "\n",
    "# Lancer l'analyse\n",
    "analyze_training_results(history, history_fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š MÃ©triques DÃ©taillÃ©es par Ã‰poque\n",
    "\n",
    "Regardons l'Ã©volution dÃ©taillÃ©e des mÃ©triques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Tableau dÃ©taillÃ© des mÃ©triques\n",
    "import pandas as pd\n",
    "\n",
    "def create_metrics_table(history1, history2=None):\n",
    "    \"\"\"\n",
    "    CrÃ©e un tableau dÃ©taillÃ© des mÃ©triques par Ã©poque\n",
    "    \"\"\"\n",
    "    # PrÃ©parer les donnÃ©es\n",
    "    if history2:\n",
    "        # Combiner les historiques\n",
    "        epochs = list(range(1, len(history1.history['accuracy']) + 1))\n",
    "        epochs.extend(range(len(history1.history['accuracy']) + 1, \n",
    "                          len(history1.history['accuracy']) + len(history2.history['accuracy']) + 1))\n",
    "        \n",
    "        train_acc = history1.history['accuracy'] + history2.history['accuracy']\n",
    "        val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
    "        train_loss = history1.history['loss'] + history2.history['loss']\n",
    "        val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
    "        \n",
    "        # Marquer les phases\n",
    "        phases = ['Transfer Learning'] * len(history1.history['accuracy'])\n",
    "        phases.extend(['Fine-Tuning'] * len(history2.history['accuracy']))\n",
    "    else:\n",
    "        epochs = list(range(1, len(history1.history['accuracy']) + 1))\n",
    "        train_acc = history1.history['accuracy']\n",
    "        val_acc = history1.history['val_accuracy']\n",
    "        train_loss = history1.history['loss']\n",
    "        val_loss = history1.history['val_loss']\n",
    "        phases = ['Training'] * len(history1.history['accuracy'])\n",
    "    \n",
    "    # CrÃ©er le DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Ã‰poque': epochs,\n",
    "        'Phase': phases,\n",
    "        'PrÃ©cision Train': [f\"{acc:.4f}\" for acc in train_acc],\n",
    "        'PrÃ©cision Val': [f\"{acc:.4f}\" for acc in val_acc],\n",
    "        'Perte Train': [f\"{loss:.4f}\" for loss in train_loss],\n",
    "        'Perte Val': [f\"{loss:.4f}\" for loss in val_loss],\n",
    "        'Ã‰cart PrÃ©cision': [f\"{train_acc[i] - val_acc[i]:.4f}\" for i in range(len(train_acc))]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# CrÃ©er et afficher le tableau\n",
    "print(\"ğŸ“‹ Tableau dÃ©taillÃ© des mÃ©triques:\")\n",
    "metrics_df = create_metrics_table(history, history_fine)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Statistiques rÃ©sumÃ©es\n",
    "print(\"\\nğŸ“Š Statistiques rÃ©sumÃ©es:\")\n",
    "phase1_df = metrics_df[metrics_df['Phase'] == 'Transfer Learning']\n",
    "phase2_df = metrics_df[metrics_df['Phase'] == 'Fine-Tuning']\n",
    "\n",
    "if len(phase2_df) > 0:\n",
    "    print(f\"Phase 1 - Meilleure prÃ©cision validation: {phase1_df['PrÃ©cision Val'].max()}\")\n",
    "    print(f\"Phase 2 - Meilleure prÃ©cision validation: {phase2_df['PrÃ©cision Val'].max()}\")\n",
    "    print(f\"AmÃ©lioration totale: {float(phase2_df['PrÃ©cision Val'].iloc[-1]) - float(phase1_df['PrÃ©cision Val'].iloc[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Ã‰valuation et PrÃ©dictions du ModÃ¨le\n",
    "\n",
    "Maintenant que notre modÃ¨le est entraÃ®nÃ©, Ã©valuons ses performances et voyons comment il fait des prÃ©dictions sur de nouvelles images.\n",
    "\n",
    "### ğŸ“Š Pourquoi Ã‰valuer ?\n",
    "\n",
    "L'Ã©valuation nous permet de :\n",
    "- **Mesurer les performances rÃ©elles** sur des donnÃ©es non vues\n",
    "- **Identifier les erreurs** de classification\n",
    "- **Comprendre les forces et faiblesses** du modÃ¨le\n",
    "- **Valider l'utilisabilitÃ©** en production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Ã‰valuation finale du modÃ¨le\n",
    "print(\"ğŸ¯ === Ã‰VALUATION FINALE DU MODÃˆLE ===\")\n",
    "print(\"\\nğŸ“‹ Ã‰valuation sur le dataset de validation...\")\n",
    "\n",
    "# Ã‰valuation dÃ©taillÃ©e\n",
    "test_loss, test_acc = model.evaluate(ds_val_processed, verbose=1)\n",
    "\n",
    "print(f\"\\nâœ… RÃ©sultats finaux:\")\n",
    "print(f\"   ğŸ“‰ Perte finale: {test_loss:.4f}\")\n",
    "print(f\"   ğŸ¯ PrÃ©cision finale: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "# InterprÃ©tation des rÃ©sultats\n",
    "if test_acc >= 0.95:\n",
    "    print(\"   ğŸŒŸ Excellente performance ! ModÃ¨le prÃªt pour la production\")\n",
    "elif test_acc >= 0.90:\n",
    "    print(\"   ğŸ‰ TrÃ¨s bonne performance ! RÃ©sultats impressionnants\")\n",
    "elif test_acc >= 0.80:\n",
    "    print(\"   ğŸ‘ Bonne performance, utilisable dans la plupart des cas\")\n",
    "elif test_acc >= 0.70:\n",
    "    print(\"   ğŸ“ˆ Performance correcte, peut Ãªtre amÃ©liorÃ©e\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Performance Ã  amÃ©liorer, rÃ©viser l'approche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Analyse des PrÃ©dictions\n",
    "\n",
    "Regardons comment notre modÃ¨le fait ses prÃ©dictions sur des exemples concrets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_show(dataset, model, num_images=9, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Affiche des prÃ©dictions du modÃ¨le avec analyse dÃ©taillÃ©e\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset de validation\n",
    "        model: ModÃ¨le entraÃ®nÃ©\n",
    "        num_images: Nombre d'images Ã  afficher\n",
    "        figsize: Taille de la figure\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.suptitle(\"ğŸ” PrÃ©dictions du ModÃ¨le - Classification de Fleurs\", \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Statistiques pour l'analyse\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    confidence_scores = []\n",
    "    \n",
    "    for images, labels in dataset.take(1):\n",
    "        # Faire les prÃ©dictions\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        \n",
    "        for i in range(min(num_images, len(images))):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            \n",
    "            # PrÃ©parer l'image pour l'affichage\n",
    "            image = images[i].numpy()\n",
    "            # DÃ©normaliser de [-1, 1] vers [0, 1]\n",
    "            image = (image + 1) / 2\n",
    "            plt.imshow(image)\n",
    "            \n",
    "            # Analyser la prÃ©diction\n",
    "            pred_probs = predictions[i]\n",
    "            pred_label = np.argmax(pred_probs)\n",
    "            true_label = labels[i].numpy()\n",
    "            confidence = pred_probs[pred_label]\n",
    "            \n",
    "            # Statistiques\n",
    "            total_predictions += 1\n",
    "            confidence_scores.append(confidence)\n",
    "            if pred_label == true_label:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "            # Couleur selon la justesse de la prÃ©diction\n",
    "            color = 'green' if pred_label == true_label else 'red'\n",
    "            \n",
    "            # Titre avec informations dÃ©taillÃ©es\n",
    "            true_name = class_names[true_label]\n",
    "            pred_name = class_names[pred_label]\n",
    "            \n",
    "            title = f\"âœ… Vrai: {true_name}\\nğŸ¯ PrÃ©dit: {pred_name}\\nğŸ“Š Confiance: {confidence:.3f}\"\n",
    "            if pred_label != true_label:\n",
    "                title = f\"âŒ Vrai: {true_name}\\nğŸ¯ PrÃ©dit: {pred_name}\\nğŸ“Š Confiance: {confidence:.3f}\"\n",
    "            \n",
    "            plt.title(title, fontsize=10, color=color, fontweight='bold')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    avg_confidence = np.mean(confidence_scores)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Statistiques sur ce batch:\")\n",
    "    print(f\"   ğŸ¯ PrÃ©cision: {correct_predictions}/{total_predictions} ({accuracy*100:.1f}%)\")\n",
    "    print(f\"   ğŸ“ˆ Confiance moyenne: {avg_confidence:.3f} ({avg_confidence*100:.1f}%)\")\n",
    "    print(f\"   ğŸ” Confiance min/max: {min(confidence_scores):.3f} / {max(confidence_scores):.3f}\")\n",
    "    \n",
    "    return correct_predictions, total_predictions, confidence_scores\n",
    "\n",
    "print(\"ğŸ” PrÃ©dictions sur le dataset de validation:\")\n",
    "correct, total, confidences = predict_and_show(ds_val_processed, model)\n",
    "\n",
    "print(\"\\nğŸ’¡ Comment interprÃ©ter ces rÃ©sultats ?\")\n",
    "print(\"   âœ… PrÃ©dictions correctes en VERT\")\n",
    "print(\"   âŒ PrÃ©dictions incorrectes en ROUGE\")\n",
    "print(\"   ğŸ“Š La confiance indique la certitude du modÃ¨le (0-1)\")\n",
    "print(\"   ğŸ¯ Une confiance > 0.8 indique une prÃ©diction trÃ¨s fiable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Matrice de Confusion\n",
    "\n",
    "La matrice de confusion nous montre en dÃ©tail quelles classes sont confondues par le modÃ¨le :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def create_confusion_matrix(dataset, model):\n",
    "    \"\"\"\n",
    "    CrÃ©e et affiche la matrice de confusion\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š CrÃ©ation de la matrice de confusion...\")\n",
    "    \n",
    "    # Collecter toutes les prÃ©dictions et vraies Ã©tiquettes\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    for images, labels in dataset:\n",
    "        # PrÃ©dictions du modÃ¨le\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        pred_labels = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        # Stocker les rÃ©sultats\n",
    "        all_predictions.extend(pred_labels)\n",
    "        all_true_labels.extend(labels.numpy())\n",
    "    \n",
    "    # CrÃ©er la matrice de confusion\n",
    "    cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Nombre de prÃ©dictions'})\n",
    "    \n",
    "    plt.title('ğŸ“Š Matrice de Confusion - Classification de Fleurs', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Classes PrÃ©dites', fontsize=12)\n",
    "    plt.ylabel('Classes RÃ©elles', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyse de la matrice\n",
    "    print(\"\\nğŸ” Analyse de la matrice de confusion:\")\n",
    "    \n",
    "    # PrÃ©cision par classe\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Vrais positifs (diagonal)\n",
    "        tp = cm[i, i]\n",
    "        # Tous les exemples de cette classe\n",
    "        total_class = np.sum(cm[i, :])\n",
    "        # PrÃ©cision pour cette classe\n",
    "        class_accuracy = tp / total_class if total_class > 0 else 0\n",
    "        \n",
    "        print(f\"   ğŸŒº {class_name}: {tp}/{total_class} ({class_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # Identifier les confusions les plus frÃ©quentes\n",
    "    print(\"\\nâš ï¸ Confusions les plus frÃ©quentes:\")\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusion_rate = cm[i, j] / np.sum(cm[i, :]) * 100\n",
    "                if confusion_rate > 10:  # Seulement les confusions > 10%\n",
    "                    print(f\"   ğŸ”„ {class_names[i]} â†’ {class_names[j]}: {cm[i, j]} cas ({confusion_rate:.1f}%)\")\n",
    "    \n",
    "    # Rapport de classification dÃ©taillÃ©\n",
    "    print(\"\\nğŸ“‹ Rapport de classification dÃ©taillÃ©:\")\n",
    "    report = classification_report(all_true_labels, all_predictions, \n",
    "                                 target_names=class_names, digits=4)\n",
    "    print(report)\n",
    "    \n",
    "    return cm, all_true_labels, all_predictions\n",
    "\n",
    "# CrÃ©er la matrice de confusion\n",
    "conf_matrix, true_labels, pred_labels = create_confusion_matrix(ds_val_processed, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Analyse des Erreurs\n",
    "\n",
    "Regardons en dÃ©tail les cas oÃ¹ le modÃ¨le se trompe pour comprendre ses limites :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(dataset, model, max_errors=6):\n",
    "    \"\"\"\n",
    "    Analyse et affiche les erreurs de classification\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” Analyse des erreurs de classification...\")\n",
    "    \n",
    "    errors_found = []\n",
    "    \n",
    "    # Chercher des erreurs dans le dataset\n",
    "    for images, labels in dataset:\n",
    "        if len(errors_found) >= max_errors:\n",
    "            break\n",
    "            \n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        pred_labels = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            if len(errors_found) >= max_errors:\n",
    "                break\n",
    "                \n",
    "            if pred_labels[i] != labels[i]:\n",
    "                errors_found.append({\n",
    "                    'image': images[i],\n",
    "                    'true_label': labels[i].numpy(),\n",
    "                    'pred_label': pred_labels[i],\n",
    "                    'confidence': predictions[i][pred_labels[i]],\n",
    "                    'true_confidence': predictions[i][labels[i]],\n",
    "                    'all_probs': predictions[i]\n",
    "                })\n",
    "    \n",
    "    if not errors_found:\n",
    "        print(\"ğŸ‰ Aucune erreur trouvÃ©e dans ce batch ! ModÃ¨le parfait sur ces exemples.\")\n",
    "        return\n",
    "    \n",
    "    # Affichage des erreurs\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(\"âŒ Analyse des Erreurs de Classification\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, error in enumerate(errors_found[:6]):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Afficher l'image\n",
    "        image = (error['image'].numpy() + 1) / 2  # DÃ©normaliser\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        # Informations sur l'erreur\n",
    "        true_name = class_names[error['true_label']]\n",
    "        pred_name = class_names[error['pred_label']]\n",
    "        \n",
    "        title = f\"âŒ Erreur #{idx+1}\\n\"\n",
    "        title += f\"âœ… Vrai: {true_name} ({error['true_confidence']:.3f})\\n\"\n",
    "        title += f\"ğŸ¯ PrÃ©dit: {pred_name} ({error['confidence']:.3f})\"\n",
    "        \n",
    "        ax.set_title(title, fontsize=10, color='red', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Cacher les sous-graphiques vides\n",
    "    for idx in range(len(errors_found), 6):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyse statistique des erreurs\n",
    "    print(f\"\\nğŸ“Š Analyse de {len(errors_found)} erreurs:\")\n",
    "    \n",
    "    # Confiance moyenne des erreurs\n",
    "    avg_error_confidence = np.mean([e['confidence'] for e in errors_found])\n",
    "    avg_true_confidence = np.mean([e['true_confidence'] for e in errors_found])\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ Confiance moyenne des prÃ©dictions erronÃ©es: {avg_error_confidence:.3f}\")\n",
    "    print(f\"   ğŸ“‰ Confiance moyenne des vraies classes: {avg_true_confidence:.3f}\")\n",
    "    print(f\"   ğŸ“Š DiffÃ©rence de confiance: {avg_error_confidence - avg_true_confidence:.3f}\")\n",
    "    \n",
    "    # Types d'erreurs\n",
    "    error_types = {}\n",
    "    for error in errors_found:\n",
    "        true_class = class_names[error['true_label']]\n",
    "        pred_class = class_names[error['pred_label']]\n",
    "        error_type = f\"{true_class} â†’ {pred_class}\"\n",
    "        error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nğŸ”„ Types d'erreurs les plus frÃ©quents:\")\n",
    "    for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   â€¢ {error_type}: {count} cas\")\n",
    "    \n",
    "    return errors_found\n",
    "\n",
    "# Analyser les erreurs\n",
    "errors = analyze_errors(ds_val_processed, model)\n",
    "\n",
    "print(\"\\nğŸ’¡ Conseils pour amÃ©liorer le modÃ¨le:\")\n",
    "print(\"   ğŸ” Examinez les erreurs pour identifier les patterns\")\n",
    "print(\"   ğŸ“Š Les classes souvent confondues nÃ©cessitent plus de donnÃ©es\")\n",
    "print(\"   ğŸ² L'augmentation de donnÃ©es peut aider pour les cas difficiles\")\n",
    "print(\"   ğŸ§  Un modÃ¨le plus complexe peut distinguer les cas subtils\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Conseils et Bonnes Pratiques\n",
    "\n",
    "### ğŸ“š Ce que Nous Avons Appris\n",
    "\n",
    "Ã€ travers ce projet, nous avons explorÃ© :\n",
    "\n",
    "âœ… **Transfer Learning** : Utilisation d'un modÃ¨le prÃ©-entraÃ®nÃ©\n",
    "\n",
    "âœ… **Fine-Tuning** : Ajustement fin en deux phases\n",
    "\n",
    "âœ… **Data Augmentation** : Enrichissement artificiel des donnÃ©es\n",
    "\n",
    "âœ… **Ã‰valuation** : Mesure des performances et analyse des erreurs\n",
    "\n",
    "âœ… **Visualisation** : ComprÃ©hension des rÃ©sultats par les graphiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ Conseils pour AmÃ©liorer Vos ModÃ¨les\n",
    "\n",
    "#### 1. ğŸ¯ Choix du ModÃ¨le PrÃ©-entraÃ®nÃ©\n",
    "\n",
    "**Pour applications mobiles :**\n",
    "- MobileNet, MobileNetV2, MobileNetV3\n",
    "- EfficientNet (bon compromis taille/performance)\n",
    "\n",
    "**Pour performances maximales :**\n",
    "- ResNet50, ResNet101\n",
    "- EfficientNet-B4, B5, B7\n",
    "- Vision Transformers (ViT)\n",
    "\n",
    "**Pour donnÃ©es mÃ©dicales :**\n",
    "- ModÃ¨les prÃ©-entraÃ®nÃ©s sur ImageNet mÃ©dical\n",
    "- ModÃ¨les spÃ©cialisÃ©s (RadImageNet, etc.)\n",
    "\n",
    "#### 2. ğŸ² StratÃ©gies d'Augmentation de DonnÃ©es\n",
    "\n",
    "**Augmentations gÃ©omÃ©triques :**\n",
    "```python\n",
    "# Basiques\n",
    "layers.RandomFlip(\"horizontal_and_vertical\")\n",
    "layers.RandomRotation(0.3)\n",
    "layers.RandomZoom(0.2)\n",
    "layers.RandomTranslation(0.1, 0.1)\n",
    "\n",
    "# AvancÃ©es\n",
    "layers.RandomContrast(0.2)\n",
    "layers.RandomBrightness(0.2)\n",
    "```\n",
    "\n",
    "**Augmentations spÃ©cialisÃ©es :**\n",
    "- **Photos de nature** : Variations de luminositÃ©, contraste\n",
    "- **Images mÃ©dicales** : Attention aux rotations (peut changer le diagnostic)\n",
    "- **Texte/Documents** : Rotations lÃ©gÃ¨res, perspective\n",
    "\n",
    "#### 3. ğŸ”§ StratÃ©gies de Fine-Tuning\n",
    "\n",
    "**DÃ©gel progressif :**\n",
    "```python\n",
    "# Phase 1: Tout gelÃ©\n",
    "base_model.trainable = False\n",
    "\n",
    "# Phase 2: DÃ©geler les derniÃ¨res couches\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Phase 3: DÃ©geler plus de couches si nÃ©cessaire\n",
    "for layer in base_model.layers[-50:]:\n",
    "    layer.trainable = True\n",
    "```\n",
    "\n",
    "**Learning rates adaptatifs :**\n",
    "```python\n",
    "# LR diffÃ©rents pour diffÃ©rentes parties\n",
    "optimizer = tf.keras.optimizers.Adam([\n",
    "    {'params': base_model.layers[-10:], 'lr': 1e-5},\n",
    "    {'params': new_layers, 'lr': 1e-4}\n",
    "])\n",
    "```\n",
    "\n",
    "#### 4. ğŸ“Š RÃ©gularisation AvancÃ©e\n",
    "\n",
    "**Dropout adaptatif :**\n",
    "```python\n",
    "# Plus de dropout pour les couches plus larges\n",
    "x = layers.Dropout(0.3)(x)  # AprÃ¨s GlobalAveragePooling\n",
    "x = layers.Dense(512)(x)\n",
    "x = layers.Dropout(0.5)(x)  # Avant la couche finale\n",
    "```\n",
    "\n",
    "**Weight decay :**\n",
    "```python\n",
    "# L2 regularization\n",
    "layers.Dense(num_classes, \n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "```\n",
    "\n",
    "#### 5. âš¡ Optimisation des Performances\n",
    "\n",
    "**Mixed Precision Training :**\n",
    "```python\n",
    "# Pour GPUs modernes\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "```\n",
    "\n",
    "**Pipeline de donnÃ©es optimisÃ© :**\n",
    "```python\n",
    "dataset = dataset.cache()  # Cache en mÃ©moire\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš ï¸ Erreurs Courantes Ã  Ã‰viter\n",
    "\n",
    "#### ğŸš¨ Erreurs de DonnÃ©es\n",
    "- **Data leakage** : MÃªme image dans train et validation\n",
    "- **DÃ©sÃ©quilibre des classes** : Pas assez d'exemples pour certaines classes\n",
    "- **QualitÃ© des labels** : Erreurs d'Ã©tiquetage\n",
    "\n",
    "#### ğŸš¨ Erreurs d'EntraÃ®nement\n",
    "- **Learning rate trop Ã©levÃ©** : Le modÃ¨le n'apprend pas\n",
    "- **Learning rate trop faible** : EntraÃ®nement trop lent\n",
    "- **Pas assez d'Ã©poques** : Sous-apprentissage\n",
    "- **Trop d'Ã©poques** : Surapprentissage\n",
    "\n",
    "#### ğŸš¨ Erreurs d'Ã‰valuation\n",
    "- **Ã‰valuer sur le train** : Surestimation des performances\n",
    "- **Pas de test set** : Pas de mesure finale objective\n",
    "- **Optimiser sur la validation** : Biais de sÃ©lection\n",
    "\n",
    "### ğŸ¯ Prochaines Ã‰tapes\n",
    "\n",
    "Pour aller plus loin :\n",
    "\n",
    "1. **ğŸ”¬ ExpÃ©rimentez** avec d'autres architectures\n",
    "2. **ğŸ“Š Collectez plus de donnÃ©es** pour amÃ©liorer les performances\n",
    "3. **ğŸ² Testez diffÃ©rentes augmentations** spÃ©cifiques Ã  votre domaine\n",
    "4. **âš¡ Optimisez pour la production** (quantization, pruning)\n",
    "5. **ğŸš€ DÃ©ployez votre modÃ¨le** (TensorFlow Serving, TensorFlow Lite)\n",
    "\n",
    "### ğŸ“š Ressources SupplÃ©mentaires\n",
    "\n",
    "- **Documentation TensorFlow** : [tensorflow.org](https://tensorflow.org)\n",
    "- **Papers avec Code** : [paperswithcode.com](https://paperswithcode.com)\n",
    "- **TensorFlow Hub** : ModÃ¨les prÃ©-entraÃ®nÃ©s\n",
    "- **Keras Applications** : Architectures populaires\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
